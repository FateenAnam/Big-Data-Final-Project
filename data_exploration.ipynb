{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "spark_jobs.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FateenAnam/Big-Data-Final-Project/blob/main/data_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRZIkSqT__Dv"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "id": "_BcbpaYeKNlG",
        "outputId": "5670a513-eb72-4922-8414-dc3005e49040",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 294136\n",
            "drwxr-xr-x  1 root root      4096 Apr 19 13:37 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "xB4TPZH16jmR",
        "outputId": "3e9bd334-e75a-4519-d404-5f650bfb966e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write Spark Code Locally and test the Code and Save it to your repository"
      ],
      "metadata": {
        "id": "o1prQTAn7Mbu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz7dMIVJ__Dy"
      },
      "source": [
        "# Step 2. Complete Spark Jobs Below Locally. \n",
        "\n",
        "Once they work you can submit them to EMR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxUib2yt__Dy"
      },
      "source": [
        "## Job 1. Count the number of tweets.\n",
        "\n",
        "I have almost completed this for you. You still have to do the reduce and add - look into the wordcount example. But then use this as the template to finish the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDL78heZ__Dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d1ed9f-bea8-4287-8b87-3719fb3454bf"
      },
      "source": [
        "%%file schema.py\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "#create spark context. This is very important. Do this similarly for the other parts\n",
        "# Note to read a file directly from s3 into an rdd you may have to do something like this\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # replace this line with the s3 pass when testing over EMR (? check proj)\n",
        "  conf = SparkConf().setAppName('schema').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "\n",
        "  try:\n",
        "    #@todo: fix the path as required\n",
        "    sqlContext = SQLContext(sc)\n",
        "\n",
        "    df = sqlContext.read.parquet('/content/nfd_incidents_xd_seg.parquet')\n",
        "    # review the page rank example for how to use the map operation\n",
        "    # review word count for reduce and add\n",
        "    # see how we use map to parse each row\n",
        "    df.printSchema()\n",
        "    count_before = df.count()\n",
        "    # drop the rows that have empty column entries\n",
        "    df = df.dropna()\n",
        "\n",
        "    # count the number of rows after dropping empty rows\n",
        "    count_after = df.count()\n",
        "\n",
        "    # calculate the number of rows dropped\n",
        "    num_dropped = count_before - count_after\n",
        "\n",
        "    # print the count of rows before and after dropping empty rows\n",
        "    print(\"Number of rows before dropping empty rows: \", count_before)\n",
        "    print(\"Number of rows after dropping empty rows: \", count_after)\n",
        "\n",
        "    # print the number of rows dropped\n",
        "    print(\"Number of rows dropped: \", num_dropped)\n",
        "\n",
        "    # @todo: the s3 version will have to save it to correct s3 path\n",
        "    # output.repartition(1).saveAsTextFile(\"s3://vandy-bd/hw6/1_count.out\")\n",
        "\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    sc.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting schema.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test local Execution Results"
      ],
      "metadata": {
        "id": "PH2TFBJp7q0O"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3y2A34y__Dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd518c5-c840-4b70-e8fe-0fc3024a443d"
      },
      "source": [
        "# execute locally and ensure everything works. If it works you should get the 1_count.out/part-00000 file. \n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 schema.py\n",
        "# note the cell magic command %%file 1_count.py is used to create a local copy of the content of cell as a file 1_count.py on colab"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-82e1566f-221d-42fd-9211-83781e103753;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 935ms :: artifacts dl 22ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-82e1566f-221d-42fd-9211-83781e103753\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/16ms)\n",
            "23/04/22 17:41:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/22 17:41:53 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/22 17:41:53 INFO ResourceUtils: ==============================================================\n",
            "23/04/22 17:41:53 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/22 17:41:53 INFO ResourceUtils: ==============================================================\n",
            "23/04/22 17:41:53 INFO SparkContext: Submitted application: schema\n",
            "23/04/22 17:41:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/22 17:41:53 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/22 17:41:53 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/22 17:41:53 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/22 17:41:53 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/22 17:41:53 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/22 17:41:53 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/22 17:41:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/22 17:41:54 INFO Utils: Successfully started service 'sparkDriver' on port 38907.\n",
            "23/04/22 17:41:54 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/22 17:41:54 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/22 17:41:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/22 17:41:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/22 17:41:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/22 17:41:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-032ca309-9df2-41b1-a8da-335f1757d1b3\n",
            "23/04/22 17:41:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/22 17:41:54 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/22 17:41:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/22 17:41:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://c93a18a26209:4040\n",
            "23/04/22 17:41:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://c93a18a26209:38907/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://c93a18a26209:38907/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://c93a18a26209:38907/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://c93a18a26209:38907/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://c93a18a26209:38907/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://c93a18a26209:38907/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://c93a18a26209:38907/jars/com.101tec_zkclient-0.3.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://c93a18a26209:38907/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://c93a18a26209:38907/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://c93a18a26209:38907/jars/log4j_log4j-1.2.17.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://c93a18a26209:38907/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://c93a18a26209:38907/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/22 17:41:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/22 17:41:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/22 17:41:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/22 17:41:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/22 17:41:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/22 17:41:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/com.101tec_zkclient-0.3.jar\n",
            "23/04/22 17:41:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/22 17:41:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/22 17:41:55 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/log4j_log4j-1.2.17.jar\n",
            "23/04/22 17:41:55 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/22 17:41:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/22 17:41:55 INFO Executor: Starting executor ID driver on host c93a18a26209\n",
            "23/04/22 17:41:55 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/log4j_log4j-1.2.17.jar\n",
            "23/04/22 17:41:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/22 17:41:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/22 17:41:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/22 17:41:55 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/22 17:41:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/22 17:41:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/22 17:41:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/22 17:41:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/22 17:41:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/22 17:41:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/com.101tec_zkclient-0.3.jar\n",
            "23/04/22 17:41:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/22 17:41:55 INFO Executor: Fetching spark://c93a18a26209:38907/jars/log4j_log4j-1.2.17.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO TransportClientFactory: Successfully created connection to c93a18a26209/172.28.0.12:38907 after 58 ms (0 ms spent in bootstraps)\n",
            "23/04/22 17:41:55 INFO Utils: Fetching spark://c93a18a26209:38907/jars/log4j_log4j-1.2.17.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp6708804029991850417.tmp\n",
            "23/04/22 17:41:55 INFO Utils: /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp6708804029991850417.tmp has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/log4j_log4j-1.2.17.jar\n",
            "23/04/22 17:41:55 INFO Executor: Adding file:/tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/22 17:41:55 INFO Executor: Fetching spark://c93a18a26209:38907/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Fetching spark://c93a18a26209:38907/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp3498908762816886016.tmp\n",
            "23/04/22 17:41:55 INFO Utils: /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp3498908762816886016.tmp has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/22 17:41:55 INFO Executor: Adding file:/tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/22 17:41:55 INFO Executor: Fetching spark://c93a18a26209:38907/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Fetching spark://c93a18a26209:38907/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp2357391540273685317.tmp\n",
            "23/04/22 17:41:55 INFO Utils: /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp2357391540273685317.tmp has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/22 17:41:55 INFO Executor: Adding file:/tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/22 17:41:55 INFO Executor: Fetching spark://c93a18a26209:38907/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Fetching spark://c93a18a26209:38907/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp188265552470844034.tmp\n",
            "23/04/22 17:41:55 INFO Utils: /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp188265552470844034.tmp has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/22 17:41:55 INFO Executor: Adding file:/tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/22 17:41:55 INFO Executor: Fetching spark://c93a18a26209:38907/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Fetching spark://c93a18a26209:38907/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp2920385058234987420.tmp\n",
            "23/04/22 17:41:55 INFO Utils: /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp2920385058234987420.tmp has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/22 17:41:55 INFO Executor: Adding file:/tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/22 17:41:55 INFO Executor: Fetching spark://c93a18a26209:38907/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Fetching spark://c93a18a26209:38907/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp2534622108338792110.tmp\n",
            "23/04/22 17:41:55 INFO Utils: /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp2534622108338792110.tmp has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/22 17:41:55 INFO Executor: Adding file:/tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/22 17:41:55 INFO Executor: Fetching spark://c93a18a26209:38907/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Fetching spark://c93a18a26209:38907/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp2609058609172655882.tmp\n",
            "23/04/22 17:41:55 INFO Utils: /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp2609058609172655882.tmp has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/22 17:41:55 INFO Executor: Adding file:/tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/22 17:41:55 INFO Executor: Fetching spark://c93a18a26209:38907/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:55 INFO Utils: Fetching spark://c93a18a26209:38907/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp4036301174186550012.tmp\n",
            "23/04/22 17:41:55 INFO Utils: /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp4036301174186550012.tmp has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/22 17:41:55 INFO Executor: Adding file:/tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/22 17:41:56 INFO Executor: Fetching spark://c93a18a26209:38907/jars/com.101tec_zkclient-0.3.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:56 INFO Utils: Fetching spark://c93a18a26209:38907/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp4563511623424999887.tmp\n",
            "23/04/22 17:41:56 INFO Utils: /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp4563511623424999887.tmp has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/com.101tec_zkclient-0.3.jar\n",
            "23/04/22 17:41:56 INFO Executor: Adding file:/tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/22 17:41:56 INFO Executor: Fetching spark://c93a18a26209:38907/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:56 INFO Utils: Fetching spark://c93a18a26209:38907/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp8698141752986419006.tmp\n",
            "23/04/22 17:41:56 INFO Utils: /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp8698141752986419006.tmp has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/22 17:41:56 INFO Executor: Adding file:/tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/22 17:41:56 INFO Executor: Fetching spark://c93a18a26209:38907/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:56 INFO Utils: Fetching spark://c93a18a26209:38907/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp7024056728538619530.tmp\n",
            "23/04/22 17:41:56 INFO Utils: /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp7024056728538619530.tmp has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/22 17:41:56 INFO Executor: Adding file:/tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/22 17:41:56 INFO Executor: Fetching spark://c93a18a26209:38907/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682185313519\n",
            "23/04/22 17:41:56 INFO Utils: Fetching spark://c93a18a26209:38907/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp5977439776579901011.tmp\n",
            "23/04/22 17:41:56 INFO Utils: /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/fetchFileTemp5977439776579901011.tmp has been previously copied to /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/22 17:41:56 INFO Executor: Adding file:/tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/userFiles-c2aef127-6711-4b3f-baa9-b6ca31197170/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/22 17:41:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37983.\n",
            "23/04/22 17:41:56 INFO NettyBlockTransferService: Server created on c93a18a26209:37983\n",
            "23/04/22 17:41:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/22 17:41:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c93a18a26209, 37983, None)\n",
            "23/04/22 17:41:56 INFO BlockManagerMasterEndpoint: Registering block manager c93a18a26209:37983 with 366.3 MiB RAM, BlockManagerId(driver, c93a18a26209, 37983, None)\n",
            "23/04/22 17:41:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c93a18a26209, 37983, None)\n",
            "23/04/22 17:41:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c93a18a26209, 37983, None)\n",
            "/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "23/04/22 17:41:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/22 17:41:57 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/22 17:41:58 INFO InMemoryFileIndex: It took 53 ms to list leaf files for 1 paths.\n",
            "23/04/22 17:41:59 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 17:41:59 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 17:41:59 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 17:41:59 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 17:41:59 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 17:41:59 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 17:41:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 105.0 KiB, free 366.2 MiB)\n",
            "23/04/22 17:41:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/22 17:41:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c93a18a26209:37983 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/22 17:41:59 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 17:41:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 17:41:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/22 17:42:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()\n",
            "23/04/22 17:42:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/22 17:42:02 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2231 bytes result sent to driver\n",
            "23/04/22 17:42:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2262 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 17:42:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/22 17:42:02 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.770 s\n",
            "23/04/22 17:42:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 17:42:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/22 17:42:02 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.984675 s\n",
            "23/04/22 17:42:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on c93a18a26209:37983 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "root\n",
            " |-- ID_Original: string (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- emdCardNumber: string (nullable = true)\n",
            " |-- time_utc: timestamp (nullable = true)\n",
            " |-- time_local: timestamp (nullable = true)\n",
            " |-- response_time_sec: double (nullable = true)\n",
            " |-- day_of_week: long (nullable = true)\n",
            " |-- weekend_or_not: integer (nullable = true)\n",
            " |-- geometry: string (nullable = true)\n",
            " |-- Incident_ID: integer (nullable = true)\n",
            " |-- Dist_to_Seg: double (nullable = true)\n",
            " |-- XDSegID: double (nullable = true)\n",
            "\n",
            "23/04/22 17:42:06 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 17:42:06 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/22 17:42:06 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
            "23/04/22 17:42:06 INFO CodeGenerator: Code generated in 269.628493 ms\n",
            "23/04/22 17:42:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 352.2 KiB, free 366.0 MiB)\n",
            "23/04/22 17:42:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 365.9 MiB)\n",
            "23/04/22 17:42:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c93a18a26209:37983 (size: 35.5 KiB, free: 366.3 MiB)\n",
            "23/04/22 17:42:07 INFO SparkContext: Created broadcast 1 from count at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 17:42:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 17:42:07 INFO DAGScheduler: Registering RDD 5 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/22 17:42:07 INFO DAGScheduler: Got map stage job 1 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 17:42:07 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 17:42:07 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 17:42:07 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 17:42:07 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 17:42:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.2 KiB, free 365.9 MiB)\n",
            "23/04/22 17:42:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.9 MiB)\n",
            "23/04/22 17:42:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on c93a18a26209:37983 (size: 7.2 KiB, free: 366.3 MiB)\n",
            "23/04/22 17:42:07 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 17:42:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 17:42:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/22 17:42:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 17:42:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/22 17:42:07 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 17:42:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2167 bytes result sent to driver\n",
            "23/04/22 17:42:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 452 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 17:42:07 INFO DAGScheduler: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.557 s\n",
            "23/04/22 17:42:07 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 17:42:07 INFO DAGScheduler: running: Set()\n",
            "23/04/22 17:42:07 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 17:42:07 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 17:42:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/22 17:42:07 INFO CodeGenerator: Code generated in 36.337895 ms\n",
            "23/04/22 17:42:08 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 17:42:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.0 KiB, free 365.9 MiB)\n",
            "23/04/22 17:42:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.9 MiB)\n",
            "23/04/22 17:42:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on c93a18a26209:37983 (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/22 17:42:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 17:42:08 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/22 17:42:08 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 17:42:08 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/22 17:42:08 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 17:42:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 35 ms\n",
            "23/04/22 17:42:08 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2648 bytes result sent to driver\n",
            "23/04/22 17:42:08 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 132 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 17:42:08 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/22 17:42:08 INFO DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.163 s\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 17:42:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 0.211026 s\n",
            "23/04/22 17:42:08 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 17:42:08 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 17:42:08 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 17:42:08 INFO CodeGenerator: Code generated in 100.535819 ms\n",
            "23/04/22 17:42:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 355.7 KiB, free 365.5 MiB)\n",
            "23/04/22 17:42:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 365.5 MiB)\n",
            "23/04/22 17:42:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on c93a18a26209:37983 (size: 36.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 17:42:08 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 17:42:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Registering RDD 12 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Got map stage job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 17:42:08 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 23.3 KiB, free 365.5 MiB)\n",
            "23/04/22 17:42:08 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 365.5 MiB)\n",
            "23/04/22 17:42:08 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on c93a18a26209:37983 (size: 8.6 KiB, free: 366.2 MiB)\n",
            "23/04/22 17:42:08 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 17:42:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 17:42:08 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "23/04/22 17:42:08 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 17:42:08 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)\n",
            "23/04/22 17:42:08 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 17:42:08 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/22 17:42:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on c93a18a26209:37983 in memory (size: 35.5 KiB, free: 366.2 MiB)\n",
            "23/04/22 17:42:09 INFO BlockManagerInfo: Removed broadcast_2_piece0 on c93a18a26209:37983 in memory (size: 7.2 KiB, free: 366.3 MiB)\n",
            "23/04/22 17:42:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on c93a18a26209:37983 in memory (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/22 17:42:09 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2223 bytes result sent to driver\n",
            "23/04/22 17:42:09 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 926 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 17:42:09 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "23/04/22 17:42:09 INFO DAGScheduler: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.954 s\n",
            "23/04/22 17:42:09 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 17:42:09 INFO DAGScheduler: running: Set()\n",
            "23/04/22 17:42:09 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 17:42:09 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 17:42:09 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 17:42:09 INFO DAGScheduler: Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 17:42:09 INFO DAGScheduler: Final stage: ResultStage 6 (count at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 17:42:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/04/22 17:42:09 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 17:42:09 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[15] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 17:42:09 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.0 KiB, free 365.9 MiB)\n",
            "23/04/22 17:42:09 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.9 MiB)\n",
            "23/04/22 17:42:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on c93a18a26209:37983 (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/22 17:42:09 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 17:42:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[15] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 17:42:09 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/04/22 17:42:09 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 17:42:09 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)\n",
            "23/04/22 17:42:09 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 17:42:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
            "23/04/22 17:42:09 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 2691 bytes result sent to driver\n",
            "23/04/22 17:42:09 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 29 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 17:42:09 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/04/22 17:42:09 INFO DAGScheduler: ResultStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.048 s\n",
            "23/04/22 17:42:09 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 17:42:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "23/04/22 17:42:09 INFO DAGScheduler: Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.062360 s\n",
            "Number of rows before dropping empty rows:  29765\n",
            "Number of rows after dropping empty rows:  21060\n",
            "Number of rows dropped:  8705\n",
            "23/04/22 17:42:09 INFO SparkUI: Stopped Spark web UI at http://c93a18a26209:4040\n",
            "23/04/22 17:42:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/22 17:42:09 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/22 17:42:09 INFO BlockManager: BlockManager stopped\n",
            "23/04/22 17:42:09 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/22 17:42:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/22 17:42:09 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/22 17:42:10 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/22 17:42:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d\n",
            "23/04/22 17:42:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-0ebf4f48-f78a-4965-8af0-469799ce61f8\n",
            "23/04/22 17:42:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-a897c493-fe14-453d-98a7-3cd5b9f6932d/pyspark-995cc7e8-d300-471a-a734-2ee27d10e679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrkWukH-__Dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce3a318-4464-4b97-9e0b-f8ffb8d82a92"
      },
      "source": [
        "def test1(lines):\n",
        "  if '6294' in lines[0] and 'correct' in lines[0]:\n",
        "    print(\"passed\")\n",
        "  else:\n",
        "    print(\"failed\")\n",
        "\n",
        "# test local execution results\n",
        "with open('1_count.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)\n",
        "  test1(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"('correct', 6294)\\n\"]\n",
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy9ZZT06__Dy"
      },
      "source": [
        "### Please save the output of each job as a single text file into your S3 bucket.\n",
        "\n",
        "Hint:\n",
        "\n",
        "1. You may call the **saveAsTextFile** function to populate the output file. \n",
        "2. Note spark may generate multiple output files due to partitioning, you can use the **repartition** or **coalesce** function to merge them to a single one.\n",
        "\n",
        "**You need to replace all s3 uri shown in below cells with yours.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNPsauvU__Dz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuvRtrzh__Dz"
      },
      "source": [
        "# upload script to S3. This assumes that your bucket name is vandy-bigdata. if not then change the  paths here.\n",
        "s3.upload_file(Filename='1_count.py', Bucket='vandy-bd', Key='hw6/1_count.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aGPm_qa__Dz"
      },
      "source": [
        "# submit spark job to emr. Make all the necessary changes to the path\n",
        "submit_job(app_name='1_count', pyfile_uri='s3://vandy-bd/hw6/1_count.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXU9mXRv__D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0241c8-0faa-4513-bf11-84eb7c4cf76a"
      },
      "source": [
        "# test EMR execution results. Once again, make sure that S3 paths are consistent.\n",
        "output_key = \"hw6/1_count.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bd', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test1(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BPCSIi0__D0"
      },
      "source": [
        "## Job 2. Count the screen name with the most tweets and its counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvw5wFdq__D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8efc31-6913-4c45-c7ce-9ca13ba18230"
      },
      "source": [
        "%%file 2_group.py\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "'''\n",
        "TODO:\n",
        "Count the screen name with the most tweets and its counts.\n",
        "\n",
        "See http://mike.teczno.com/notes/streaming-data-from-twitter.html for parsing info.\n",
        "Get the screen name by accessing tweet['user']['screen_name']\n",
        "\n",
        "Output:\n",
        "number_of_most_tweets    username\n",
        "\n",
        "'''\n",
        "def process(entry):\n",
        "  try:\n",
        "    tweet = json.loads(entry)\n",
        "    #if load succeeded. We use correct as the key\n",
        "    name = tweet['user']['screen_name']\n",
        "    return name, 1\n",
        "  except:\n",
        "    #there was an error in loading. We use incorrect as the key\n",
        "    return \"incorrect\", 0\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    conf = SparkConf().setAppName('2_group').set('spark.hadoop.validateOutputSpecs', False)\n",
        "    sc = SparkContext(conf=conf).getOrCreate()\n",
        "    try:\n",
        "      #@todo: fix the path as required\n",
        "      tweets=sc.textFile('s3://vandy-bd/hw6/nashville-tweets-2019-01-28')\n",
        "      # review the page rank example for how to use the map operation\n",
        "      # review word count for reduce and add\n",
        "      # see how we use map to parse each row\n",
        "      tweetinfo = tweets.map(lambda tweet: process(tweet))\n",
        "\n",
        "      # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "      counts = tweetinfo.reduceByKey(lambda x, y: x + y)\n",
        "      output = counts.reduce(lambda x, y: x if x[1] > y[1] else y)\n",
        "      output = sc.parallelize([output])\n",
        "      # @todo: the s3 version will have to save it to correct s3 path\n",
        "      output.repartition(1).saveAsTextFile(\"s3://vandy-bd/hw6/2_group.out\")\n",
        "\n",
        "    finally:\n",
        "      # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "      #finally is used to make sure the context is stopped even with errors\n",
        "      sc.stop()\n",
        "\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 2_group.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute and test locally"
      ],
      "metadata": {
        "id": "1xe2ZqP18Uso"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH6TK-Nk__D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58fafa7d-099a-4500-d889-ff0c52eadc2c"
      },
      "source": [
        "\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 2_group.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-07408ff3-55fc-46dc-9a6f-b67ff36fc606;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 784ms :: artifacts dl 33ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-07408ff3-55fc-46dc-9a6f-b67ff36fc606\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/21ms)\n",
            "23/04/11 17:47:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/11 17:47:58 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/11 17:47:59 INFO ResourceUtils: ==============================================================\n",
            "23/04/11 17:47:59 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/11 17:47:59 INFO ResourceUtils: ==============================================================\n",
            "23/04/11 17:47:59 INFO SparkContext: Submitted application: 2_group\n",
            "23/04/11 17:47:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/11 17:47:59 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/11 17:47:59 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/11 17:47:59 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/11 17:47:59 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/11 17:47:59 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/11 17:47:59 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/11 17:47:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/11 17:47:59 INFO Utils: Successfully started service 'sparkDriver' on port 34497.\n",
            "23/04/11 17:47:59 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/11 17:47:59 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/11 17:47:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/11 17:47:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/11 17:47:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/11 17:47:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ddc062b7-2fc2-47fa-a4a7-b3e947c77bd3\n",
            "23/04/11 17:47:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/11 17:47:59 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/11 17:48:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/11 17:48:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ed3eecdcf213:4040\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://ed3eecdcf213:34497/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://ed3eecdcf213:34497/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://ed3eecdcf213:34497/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://ed3eecdcf213:34497/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://ed3eecdcf213:34497/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://ed3eecdcf213:34497/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://ed3eecdcf213:34497/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://ed3eecdcf213:34497/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://ed3eecdcf213:34497/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://ed3eecdcf213:34497/jars/log4j_log4j-1.2.17.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://ed3eecdcf213:34497/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://ed3eecdcf213:34497/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/log4j_log4j-1.2.17.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 17:48:00 INFO Executor: Starting executor ID driver on host ed3eecdcf213\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/log4j_log4j-1.2.17.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO TransportClientFactory: Successfully created connection to ed3eecdcf213/172.28.0.12:34497 after 59 ms (0 ms spent in bootstraps)\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3084373140676113814.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3084373140676113814.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp2079614968253254215.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp2079614968253254215.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp4006233683702446428.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp4006233683702446428.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp8490424229472509464.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp8490424229472509464.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp1253708418988046172.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp1253708418988046172.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp4385509867363552970.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp4385509867363552970.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3217525169282415113.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3217525169282415113.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3390783527352272984.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3390783527352272984.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp1487910310076018304.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp1487910310076018304.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/log4j_log4j-1.2.17.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/log4j_log4j-1.2.17.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp6083834070089690532.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp6083834070089690532.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/log4j_log4j-1.2.17.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3346549257331982440.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3346549257331982440.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp1303077820120664359.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp1303077820120664359.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/11 17:48:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39279.\n",
            "23/04/11 17:48:01 INFO NettyBlockTransferService: Server created on ed3eecdcf213:39279\n",
            "23/04/11 17:48:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/11 17:48:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ed3eecdcf213, 39279, None)\n",
            "23/04/11 17:48:01 INFO BlockManagerMasterEndpoint: Registering block manager ed3eecdcf213:39279 with 366.3 MiB RAM, BlockManagerId(driver, ed3eecdcf213, 39279, None)\n",
            "23/04/11 17:48:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ed3eecdcf213, 39279, None)\n",
            "23/04/11 17:48:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ed3eecdcf213, 39279, None)\n",
            "23/04/11 17:48:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/11 17:48:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ed3eecdcf213:39279 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:48:02 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/11 17:48:02 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/11 17:48:03 INFO SparkContext: Starting job: reduce at /content/2_group.py:38\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/2_group.py:37) as input to shuffle 0\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Got job 0 (reduce at /content/2_group.py:38) with 2 output partitions\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Final stage: ResultStage 1 (reduce at /content/2_group.py:38)\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/2_group.py:37), which has no missing parents\n",
            "23/04/11 17:48:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ed3eecdcf213:39279 (size: 7.6 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:48:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/2_group.py:37) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/11 17:48:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "23/04/11 17:48:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ed3eecdcf213, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:03 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (ed3eecdcf213, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/11 17:48:03 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "23/04/11 17:48:04 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/11 17:48:04 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/11 17:48:07 INFO PythonRunner: Times: total = 2190, boot = 1322, init = 61, finish = 807\n",
            "23/04/11 17:48:07 INFO PythonRunner: Times: total = 2326, boot = 1322, init = 65, finish = 939\n",
            "23/04/11 17:48:07 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1612 bytes result sent to driver\n",
            "23/04/11 17:48:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1655 bytes result sent to driver\n",
            "23/04/11 17:48:07 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 4129 ms on ed3eecdcf213 (executor driver) (1/2)\n",
            "23/04/11 17:48:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4229 ms on ed3eecdcf213 (executor driver) (2/2)\n",
            "23/04/11 17:48:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:48:07 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 35103\n",
            "23/04/11 17:48:08 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/2_group.py:37) finished in 4.729 s\n",
            "23/04/11 17:48:08 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/11 17:48:08 INFO DAGScheduler: running: Set()\n",
            "23/04/11 17:48:08 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "23/04/11 17:48:08 INFO DAGScheduler: failed: Set()\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at reduce at /content/2_group.py:38), which has no missing parents\n",
            "23/04/11 17:48:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.6 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ed3eecdcf213:39279 (size: 6.2 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:48:08 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at reduce at /content/2_group.py:38) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/11 17:48:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (ed3eecdcf213, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (ed3eecdcf213, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:08 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
            "23/04/11 17:48:08 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n",
            "23/04/11 17:48:08 INFO ShuffleBlockFetcherIterator: Getting 2 (21.2 KiB) non-empty blocks including 2 (21.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 17:48:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 40 ms\n",
            "23/04/11 17:48:08 INFO ShuffleBlockFetcherIterator: Getting 2 (21.2 KiB) non-empty blocks including 2 (21.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 17:48:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 50 ms\n",
            "23/04/11 17:48:08 INFO PythonRunner: Times: total = 23, boot = -918, init = 937, finish = 4\n",
            "23/04/11 17:48:08 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1623 bytes result sent to driver\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 212 ms on ed3eecdcf213 (executor driver) (1/2)\n",
            "23/04/11 17:48:08 INFO PythonRunner: Times: total = 53, boot = -775, init = 825, finish = 3\n",
            "23/04/11 17:48:08 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1627 bytes result sent to driver\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 243 ms on ed3eecdcf213 (executor driver) (2/2)\n",
            "23/04/11 17:48:08 INFO DAGScheduler: ResultStage 1 (reduce at /content/2_group.py:38) finished in 0.271 s\n",
            "23/04/11 17:48:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:48:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/11 17:48:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Job 0 finished: reduce at /content/2_group.py:38, took 5.174936 s\n",
            "23/04/11 17:48:08 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/11 17:48:08 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/11 17:48:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/11 17:48:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/11 17:48:08 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Registering RDD 9 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Got job 1 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/11 17:48:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.8 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ed3eecdcf213:39279 (size: 4.8 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:48:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/11 17:48:08 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (ed3eecdcf213, executor driver, partition 0, PROCESS_LOCAL, 4422 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5) (ed3eecdcf213, executor driver, partition 1, PROCESS_LOCAL, 4460 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:08 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)\n",
            "23/04/11 17:48:08 INFO Executor: Running task 1.0 in stage 2.0 (TID 5)\n",
            "23/04/11 17:48:08 INFO PythonRunner: Times: total = 44, boot = -353, init = 397, finish = 0\n",
            "23/04/11 17:48:08 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 1353 bytes result sent to driver\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 103 ms on ed3eecdcf213 (executor driver) (1/2)\n",
            "23/04/11 17:48:08 INFO PythonRunner: Times: total = 53, boot = -365, init = 418, finish = 0\n",
            "23/04/11 17:48:08 INFO Executor: Finished task 1.0 in stage 2.0 (TID 5). 1482 bytes result sent to driver\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 122 ms on ed3eecdcf213 (executor driver) (2/2)\n",
            "23/04/11 17:48:08 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:48:08 INFO DAGScheduler: ShuffleMapStage 2 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.148 s\n",
            "23/04/11 17:48:08 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/11 17:48:08 INFO DAGScheduler: running: Set()\n",
            "23/04/11 17:48:08 INFO DAGScheduler: waiting: Set(ResultStage 3)\n",
            "23/04/11 17:48:08 INFO DAGScheduler: failed: Set()\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/11 17:48:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 105.9 KiB, free 365.8 MiB)\n",
            "23/04/11 17:48:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.7 MiB)\n",
            "23/04/11 17:48:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ed3eecdcf213:39279 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/11 17:48:08 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/11 17:48:08 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6) (ed3eecdcf213, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:08 INFO Executor: Running task 0.0 in stage 3.0 (TID 6)\n",
            "23/04/11 17:48:08 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/11 17:48:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/11 17:48:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/11 17:48:08 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 17:48:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/04/11 17:48:09 INFO PythonRunner: Times: total = 43, boot = -204, init = 247, finish = 0\n",
            "23/04/11 17:48:09 INFO FileOutputCommitter: Saved output of task 'attempt_202304111748087265173027041852046_0015_m_000000_0' to file:/content/2_group.out/_temporary/0/task_202304111748087265173027041852046_0015_m_000000\n",
            "23/04/11 17:48:09 INFO SparkHadoopMapRedUtil: attempt_202304111748087265173027041852046_0015_m_000000_0: Committed\n",
            "23/04/11 17:48:09 INFO Executor: Finished task 0.0 in stage 3.0 (TID 6). 1952 bytes result sent to driver\n",
            "23/04/11 17:48:09 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 258 ms on ed3eecdcf213 (executor driver) (1/1)\n",
            "23/04/11 17:48:09 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:48:09 INFO DAGScheduler: ResultStage 3 (runJob at SparkHadoopWriter.scala:83) finished in 0.298 s\n",
            "23/04/11 17:48:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/11 17:48:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/11 17:48:09 INFO DAGScheduler: Job 1 finished: runJob at SparkHadoopWriter.scala:83, took 0.473611 s\n",
            "23/04/11 17:48:09 INFO SparkHadoopWriter: Start to commit write Job job_202304111748087265173027041852046_0015.\n",
            "23/04/11 17:48:09 INFO SparkHadoopWriter: Write Job job_202304111748087265173027041852046_0015 committed. Elapsed time: 24 ms.\n",
            "23/04/11 17:48:09 INFO SparkUI: Stopped Spark web UI at http://ed3eecdcf213:4040\n",
            "23/04/11 17:48:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/11 17:48:09 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/11 17:48:09 INFO BlockManager: BlockManager stopped\n",
            "23/04/11 17:48:09 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/11 17:48:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/11 17:48:09 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/11 17:48:10 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/11 17:48:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700\n",
            "23/04/11 17:48:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/pyspark-52414ed4-6994-4c85-ac93-04d8b80ce72d\n",
            "23/04/11 17:48:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-1f8501ae-2a1b-453f-a70d-626f08a21a42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvpqT474__D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2dc793-ed3c-496e-8c58-08caf13877e3"
      },
      "source": [
        "def test2(lines):\n",
        "    assert lines[0].strip() == \"('rpsabo', 88)\"\n",
        "    print(\"passed\")\n",
        "\n",
        "# test local execution results\n",
        "with open('2_group.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  test2(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeAaRlMR__D1"
      },
      "source": [
        "## Job 3. Count the tweets per day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTMTHj9O__D2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d6df42d-06cd-4424-94e3-1478d838b3f1"
      },
      "source": [
        "%%file 3_days.py\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "'''\n",
        "TODO:\n",
        "Count the tweets per day.\n",
        "\n",
        "See http://mike.teczno.com/notes/streaming-data-from-twitter.html for parsing info.\n",
        "Get the screen name by accessing tweet['user']['screen_name']\n",
        "\n",
        "Look at tweet['created_at'] for datetime of creation. Just use the first word in the date to get the day.\n",
        "\n",
        "'''\n",
        "\n",
        "def process(entry):\n",
        "  try:\n",
        "    tweet = json.loads(entry)\n",
        "    #if load succeeded. We use correct as the key\n",
        "    name = tweet['created_at'].split(' ')[0]\n",
        "    return name, 1\n",
        "  except:\n",
        "    #there was an error in loading. We use incorrect as the key\n",
        "    return \"incorrect\", 0\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    conf = SparkConf().setAppName('3_days').set('spark.hadoop.validateOutputSpecs', False)\n",
        "    sc = SparkContext(conf=conf).getOrCreate()\n",
        "    try:\n",
        "      #@todo: fix the path as required\n",
        "      tweets=sc.textFile('s3://vandy-bd/hw6/nashville-tweets-2019-01-28')\n",
        "      # review the page rank example for how to use the map operation\n",
        "      # review word count for reduce and add\n",
        "      # see how we use map to parse each row\n",
        "      tweetinfo = tweets.map(lambda tweet: process(tweet))\n",
        "\n",
        "      # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "      counts = tweetinfo.reduceByKey(lambda x, y: x + y)\n",
        "      # @todo: the s3 version will have to save it to correct s3 path\n",
        "      counts.repartition(1).saveAsTextFile(\"s3://vandy-bd/hw6/3_days.out\")\n",
        "\n",
        "    finally:\n",
        "      # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "      #finally is used to make sure the context is stopped even with errors\n",
        "      sc.stop()\n",
        "\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 3_days.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6phVcUW__D2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5f876a-ea0d-408e-8cb0-7c75e44f2582"
      },
      "source": [
        "# execute locally\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 3_days.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-58659cb3-689d-45dd-9bb7-c1e75c13e278;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 881ms :: artifacts dl 42ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-58659cb3-689d-45dd-9bb7-c1e75c13e278\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/29ms)\n",
            "23/04/11 17:53:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/11 17:53:45 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/11 17:53:45 INFO ResourceUtils: ==============================================================\n",
            "23/04/11 17:53:45 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/11 17:53:45 INFO ResourceUtils: ==============================================================\n",
            "23/04/11 17:53:45 INFO SparkContext: Submitted application: 3_days\n",
            "23/04/11 17:53:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/11 17:53:45 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/11 17:53:45 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/11 17:53:45 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/11 17:53:45 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/11 17:53:45 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/11 17:53:45 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/11 17:53:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/11 17:53:45 INFO Utils: Successfully started service 'sparkDriver' on port 44007.\n",
            "23/04/11 17:53:45 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/11 17:53:45 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/11 17:53:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/11 17:53:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/11 17:53:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/11 17:53:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-48d66f4a-bc2f-4889-8e52-8e26c22a9f56\n",
            "23/04/11 17:53:46 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/11 17:53:46 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/11 17:53:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/11 17:53:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ed3eecdcf213:4040\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://ed3eecdcf213:44007/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://ed3eecdcf213:44007/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://ed3eecdcf213:44007/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://ed3eecdcf213:44007/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://ed3eecdcf213:44007/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://ed3eecdcf213:44007/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://ed3eecdcf213:44007/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://ed3eecdcf213:44007/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://ed3eecdcf213:44007/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://ed3eecdcf213:44007/jars/log4j_log4j-1.2.17.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://ed3eecdcf213:44007/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://ed3eecdcf213:44007/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/log4j_log4j-1.2.17.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 17:53:47 INFO Executor: Starting executor ID driver on host ed3eecdcf213\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/log4j_log4j-1.2.17.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO TransportClientFactory: Successfully created connection to ed3eecdcf213/172.28.0.12:44007 after 49 ms (0 ms spent in bootstraps)\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp5790867608299968887.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp5790867608299968887.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp251204340706217194.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp251204340706217194.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp9079337244795208535.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp9079337244795208535.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp9111849829727596329.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp9111849829727596329.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp8761562424887191527.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp8761562424887191527.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp4680667975855305434.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp4680667975855305434.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp6497805972133494473.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp6497805972133494473.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp1601756906431295054.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp1601756906431295054.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/log4j_log4j-1.2.17.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/log4j_log4j-1.2.17.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp6295988508873001202.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp6295988508873001202.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/log4j_log4j-1.2.17.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp2111848176638070524.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp2111848176638070524.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp2915826182393640288.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp2915826182393640288.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp7933663231486743272.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp7933663231486743272.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/11 17:53:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37277.\n",
            "23/04/11 17:53:47 INFO NettyBlockTransferService: Server created on ed3eecdcf213:37277\n",
            "23/04/11 17:53:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/11 17:53:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ed3eecdcf213, 37277, None)\n",
            "23/04/11 17:53:47 INFO BlockManagerMasterEndpoint: Registering block manager ed3eecdcf213:37277 with 366.3 MiB RAM, BlockManagerId(driver, ed3eecdcf213, 37277, None)\n",
            "23/04/11 17:53:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ed3eecdcf213, 37277, None)\n",
            "23/04/11 17:53:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ed3eecdcf213, 37277, None)\n",
            "23/04/11 17:53:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/11 17:53:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/11 17:53:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ed3eecdcf213:37277 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:53:48 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/11 17:53:48 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/11 17:53:49 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/11 17:53:49 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/11 17:53:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/11 17:53:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/11 17:53:49 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/3_days.py:37) as input to shuffle 1\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Registering RDD 7 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/3_days.py:37), which has no missing parents\n",
            "23/04/11 17:53:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)\n",
            "23/04/11 17:53:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 365.9 MiB)\n",
            "23/04/11 17:53:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ed3eecdcf213:37277 (size: 7.6 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:53:49 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/3_days.py:37) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/11 17:53:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "23/04/11 17:53:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ed3eecdcf213, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:53:50 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (ed3eecdcf213, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:53:50 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "23/04/11 17:53:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/11 17:53:50 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/11 17:53:50 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/11 17:53:52 INFO PythonRunner: Times: total = 1157, boot = 645, init = 28, finish = 484\n",
            "23/04/11 17:53:52 INFO PythonRunner: Times: total = 1126, boot = 625, init = 39, finish = 462\n",
            "23/04/11 17:53:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1655 bytes result sent to driver\n",
            "23/04/11 17:53:52 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1612 bytes result sent to driver\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2333 ms on ed3eecdcf213 (executor driver) (1/2)\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2385 ms on ed3eecdcf213 (executor driver) (2/2)\n",
            "23/04/11 17:53:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:53:52 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 49107\n",
            "23/04/11 17:53:52 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/3_days.py:37) finished in 2.943 s\n",
            "23/04/11 17:53:52 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/11 17:53:52 INFO DAGScheduler: running: Set()\n",
            "23/04/11 17:53:52 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)\n",
            "23/04/11 17:53:52 INFO DAGScheduler: failed: Set()\n",
            "23/04/11 17:53:52 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/11 17:53:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.6 KiB, free 365.9 MiB)\n",
            "23/04/11 17:53:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.9 MiB)\n",
            "23/04/11 17:53:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ed3eecdcf213:37277 (size: 6.2 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:53:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:53:52 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/11 17:53:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (ed3eecdcf213, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3) (ed3eecdcf213, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:53:52 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "23/04/11 17:53:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 3)\n",
            "23/04/11 17:53:52 INFO ShuffleBlockFetcherIterator: Getting 2 (144.0 B) non-empty blocks including 2 (144.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 17:53:52 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 17:53:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 36 ms\n",
            "23/04/11 17:53:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 33 ms\n",
            "23/04/11 17:53:52 INFO PythonRunner: Times: total = 26, boot = -884, init = 909, finish = 1\n",
            "23/04/11 17:53:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 3). 1654 bytes result sent to driver\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 206 ms on ed3eecdcf213 (executor driver) (1/2)\n",
            "23/04/11 17:53:52 INFO PythonRunner: Times: total = 68, boot = -878, init = 946, finish = 0\n",
            "23/04/11 17:53:52 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1783 bytes result sent to driver\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 263 ms on ed3eecdcf213 (executor driver) (2/2)\n",
            "23/04/11 17:53:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:53:52 INFO DAGScheduler: ShuffleMapStage 1 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.341 s\n",
            "23/04/11 17:53:52 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/11 17:53:52 INFO DAGScheduler: running: Set()\n",
            "23/04/11 17:53:52 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
            "23/04/11 17:53:52 INFO DAGScheduler: failed: Set()\n",
            "23/04/11 17:53:52 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/11 17:53:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 105.9 KiB, free 365.8 MiB)\n",
            "23/04/11 17:53:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.7 MiB)\n",
            "23/04/11 17:53:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ed3eecdcf213:37277 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/11 17:53:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:53:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/11 17:53:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (ed3eecdcf213, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:53:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)\n",
            "23/04/11 17:53:53 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/11 17:53:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/11 17:53:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/11 17:53:53 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 17:53:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms\n",
            "23/04/11 17:53:53 INFO PythonRunner: Times: total = 61, boot = -402, init = 463, finish = 0\n",
            "23/04/11 17:53:53 INFO FileOutputCommitter: Saved output of task 'attempt_202304111753491786148337591500640_0013_m_000000_0' to file:/content/3_days.out/_temporary/0/task_202304111753491786148337591500640_0013_m_000000\n",
            "23/04/11 17:53:53 INFO SparkHadoopMapRedUtil: attempt_202304111753491786148337591500640_0013_m_000000_0: Committed\n",
            "23/04/11 17:53:53 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 1952 bytes result sent to driver\n",
            "23/04/11 17:53:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 443 ms on ed3eecdcf213 (executor driver) (1/1)\n",
            "23/04/11 17:53:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:53:53 INFO DAGScheduler: ResultStage 2 (runJob at SparkHadoopWriter.scala:83) finished in 0.518 s\n",
            "23/04/11 17:53:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/11 17:53:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "23/04/11 17:53:53 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 4.031658 s\n",
            "23/04/11 17:53:53 INFO SparkHadoopWriter: Start to commit write Job job_202304111753491786148337591500640_0013.\n",
            "23/04/11 17:53:53 INFO SparkHadoopWriter: Write Job job_202304111753491786148337591500640_0013 committed. Elapsed time: 45 ms.\n",
            "23/04/11 17:53:53 INFO SparkUI: Stopped Spark web UI at http://ed3eecdcf213:4040\n",
            "23/04/11 17:53:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/11 17:53:53 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/11 17:53:53 INFO BlockManager: BlockManager stopped\n",
            "23/04/11 17:53:53 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/11 17:53:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/11 17:53:53 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/11 17:53:54 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/11 17:53:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392\n",
            "23/04/11 17:53:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/pyspark-28dfbd5c-f935-42c1-ba94-28109cd82518\n",
            "23/04/11 17:53:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-d8c7a4ad-0b9b-4c0f-b93c-80458ea4dedf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h8y_YU2__D2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e8e4d0-3a5f-4136-c54f-cc9ee7ba82a2"
      },
      "source": [
        "def test3(lines):\n",
        "    if lines[0].strip() == \"('Sun', 6294)\":\n",
        "        print(\"passed\")\n",
        "    else:\n",
        "        assert False\n",
        "# test locall execution results\n",
        "with open('3_days.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  test3(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdiDy-gu__D3"
      },
      "source": [
        "## Job 4. Join the batting and salaries data for Barry Bonds per year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MIbbmfk__D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f27b2e47-3d95-4ce6-ba77-b79120ccb01b"
      },
      "source": [
        "%%file 4_join.py\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "'''\n",
        "TODO:\n",
        "Join the batting and salaries data for Barry Bonds per year.\n",
        "\n",
        "The output should be the combined CSV string of batting and salaries data (one per year).\n",
        "\n",
        "Final output format:\n",
        "E.g:\n",
        "('join', 'bondsba01,2006,1,SFN,NL,130,367,74,99,23,0,26,77,3,0,115,51,38,10,0,1,92006,SFN,NL,bondsba01,19331470')\n",
        "\n",
        "Schema:\n",
        "Salaries: yearID\tteamID\tlgID\tplayerID\tsalary\n",
        "Batting: playerID\tyearID\tstint\tteamID\tlgID\tG\tAB\tR\tH\t2B\t3B\tHR\tRBI\tSB\tCS\tBB\tSO\n",
        "\n",
        "Hints: \n",
        "Use split to split the CSV lines (e.g. s = line.split(','))\n",
        "Both files are read as text file stream. Use the length of the lines to determine which is which.\n",
        "'''\n",
        "\n",
        "def process(line):\n",
        "    s = line.split(',')\n",
        "    if len(s) == 5:\n",
        "      playerID = s[3]\n",
        "      yearID = s[0]\n",
        "      if playerID == \"bondsba01\":\n",
        "        return (playerID, yearID), (\"salaries\", line) \n",
        "    else:\n",
        "      playerID = s[0]\n",
        "      yearID = s[1]\n",
        "      if playerID == \"bondsba01\":\n",
        "        # del s[-1] don't want to filter these out this time \n",
        "        # del s[5]\n",
        "        string = ','.join(s)\n",
        "        return (playerID, yearID), (\"batting\", string) \n",
        "    pass\n",
        "\n",
        "def barryBonds(line):\n",
        "    s = line.split(',')\n",
        "    if len(s) == 5:\n",
        "      playerID = s[3]\n",
        "      return playerID == \"bondsba01\"\n",
        "    else:\n",
        "      playerID = s[0]\n",
        "      return playerID == \"bondsba01\"\n",
        "\n",
        "def reducer(x, y):\n",
        "    output = \"\"\n",
        "    if x[0] == \"salaries\":\n",
        "      output = y[1] + x[1]\n",
        "    else:\n",
        "      output = x[1] + y[1]\n",
        "    return output\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    conf = SparkConf().setAppName('4_join').set('spark.hadoop.validateOutputSpecs', False)\n",
        "    sc = SparkContext(conf=conf).getOrCreate()\n",
        "    try:\n",
        "      #@todo: fix the path as required\n",
        "      # Read in the first CSV file\n",
        "      file1 = sc.textFile('s3://vandy-bd/hw6/Batting.csv')\n",
        "\n",
        "      # Read in the second CSV file\n",
        "      file2 = sc.textFile('s3://vandy-bd/hw6/Salaries.csv')\n",
        "\n",
        "      # Union the two RDDs\n",
        "      merged_rdd = file1.union(file2)\n",
        "\n",
        "      filtered = merged_rdd.filter(lambda x: barryBonds(x))\n",
        "      # review the page rank example for how to use the map operation\n",
        "      # review word count for reduce and add\n",
        "      # see how we use map to parse each row\n",
        "      info = filtered.map(lambda line: process(line))\n",
        "\n",
        "      # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "      reduced = info.reduceByKey(lambda x, y: reducer(x,y))\n",
        "\n",
        "      output = reduced.map(lambda x: ('join', x[1]))\n",
        "      # @todo: the s3 version will have to save it to correct s3 path\n",
        "      output.repartition(1).saveAsTextFile(\"s3://vandy-bd/hw6/4_join.out\")\n",
        "\n",
        "    finally:\n",
        "      # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "      #finally is used to make sure the context is stopped even with errors\n",
        "      sc.stop()\n",
        "\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 4_join.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5_a0GAy__D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e564157-085e-4227-885d-bc45e7945f22"
      },
      "source": [
        "# execute locally\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 4_join.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-789dddd2-0248-4cd2-aebe-4e39c0cc94b3;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 1152ms :: artifacts dl 20ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-789dddd2-0248-4cd2-aebe-4e39c0cc94b3\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/14ms)\n",
            "23/04/11 18:18:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/11 18:18:58 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/11 18:18:58 INFO ResourceUtils: ==============================================================\n",
            "23/04/11 18:18:58 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/11 18:18:58 INFO ResourceUtils: ==============================================================\n",
            "23/04/11 18:18:58 INFO SparkContext: Submitted application: 4_join\n",
            "23/04/11 18:18:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/11 18:18:58 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/11 18:18:58 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/11 18:18:59 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/11 18:18:59 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/11 18:18:59 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/11 18:18:59 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/11 18:18:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/11 18:18:59 INFO Utils: Successfully started service 'sparkDriver' on port 41855.\n",
            "23/04/11 18:18:59 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/11 18:18:59 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/11 18:18:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/11 18:18:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/11 18:18:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/11 18:18:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7219cf5a-8e7f-48c9-9bb2-81e7ffa87911\n",
            "23/04/11 18:18:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/11 18:18:59 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/11 18:19:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/11 18:19:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ed3eecdcf213:4040\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://ed3eecdcf213:41855/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://ed3eecdcf213:41855/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://ed3eecdcf213:41855/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://ed3eecdcf213:41855/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://ed3eecdcf213:41855/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://ed3eecdcf213:41855/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://ed3eecdcf213:41855/jars/com.101tec_zkclient-0.3.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://ed3eecdcf213:41855/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://ed3eecdcf213:41855/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://ed3eecdcf213:41855/jars/log4j_log4j-1.2.17.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://ed3eecdcf213:41855/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://ed3eecdcf213:41855/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/log4j_log4j-1.2.17.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 18:19:01 INFO Executor: Starting executor ID driver on host ed3eecdcf213\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/log4j_log4j-1.2.17.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO TransportClientFactory: Successfully created connection to ed3eecdcf213/172.28.0.12:41855 after 61 ms (0 ms spent in bootstraps)\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp7214593820985547119.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp7214593820985547119.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/log4j_log4j-1.2.17.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/log4j_log4j-1.2.17.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp3576772516628083415.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp3576772516628083415.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/log4j_log4j-1.2.17.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp4163802919084522420.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp4163802919084522420.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp3106943651923066360.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp3106943651923066360.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/com.101tec_zkclient-0.3.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp2820697503579822926.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp2820697503579822926.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp5743389961248549584.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp5743389961248549584.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp1148893212453496091.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp1148893212453496091.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp6145049119202286918.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp6145049119202286918.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp5474905512289250990.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp5474905512289250990.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp655982983456073997.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp655982983456073997.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp4201345403417177334.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp4201345403417177334.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp5709645794656824965.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp5709645794656824965.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/11 18:19:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32799.\n",
            "23/04/11 18:19:01 INFO NettyBlockTransferService: Server created on ed3eecdcf213:32799\n",
            "23/04/11 18:19:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/11 18:19:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ed3eecdcf213, 32799, None)\n",
            "23/04/11 18:19:01 INFO BlockManagerMasterEndpoint: Registering block manager ed3eecdcf213:32799 with 366.3 MiB RAM, BlockManagerId(driver, ed3eecdcf213, 32799, None)\n",
            "23/04/11 18:19:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ed3eecdcf213, 32799, None)\n",
            "23/04/11 18:19:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ed3eecdcf213, 32799, None)\n",
            "23/04/11 18:19:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/11 18:19:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/11 18:19:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ed3eecdcf213:32799 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/11 18:19:02 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/11 18:19:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 354.3 KiB, free 365.6 MiB)\n",
            "23/04/11 18:19:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.5 MiB)\n",
            "23/04/11 18:19:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ed3eecdcf213:32799 (size: 32.0 KiB, free: 366.2 MiB)\n",
            "23/04/11 18:19:03 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/11 18:19:03 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/11 18:19:03 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/11 18:19:03 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/11 18:19:03 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/11 18:19:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/11 18:19:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/11 18:19:03 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Registering RDD 6 (reduceByKey at /content/4_join.py:78) as input to shuffle 1\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Registering RDD 10 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[6] at reduceByKey at /content/4_join.py:78), which has no missing parents\n",
            "23/04/11 18:19:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.0 KiB, free 365.5 MiB)\n",
            "23/04/11 18:19:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 365.5 MiB)\n",
            "23/04/11 18:19:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ed3eecdcf213:32799 (size: 8.4 KiB, free: 366.2 MiB)\n",
            "23/04/11 18:19:04 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 18:19:04 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 0 (PairwiseRDD[6] at reduceByKey at /content/4_join.py:78) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "23/04/11 18:19:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks resource profile 0\n",
            "23/04/11 18:19:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ed3eecdcf213, executor driver, partition 0, PROCESS_LOCAL, 4592 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:04 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (ed3eecdcf213, executor driver, partition 1, PROCESS_LOCAL, 4592 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/11 18:19:04 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "23/04/11 18:19:04 INFO HadoopRDD: Input split: file:/content/Batting.csv:3244373+3244374\n",
            "23/04/11 18:19:04 INFO HadoopRDD: Input split: file:/content/Batting.csv:0+3244373\n",
            "23/04/11 18:19:06 INFO PythonRunner: Times: total = 1135, boot = 680, init = 43, finish = 412\n",
            "23/04/11 18:19:06 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1485 bytes result sent to driver\n",
            "23/04/11 18:19:06 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (ed3eecdcf213, executor driver, partition 2, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:06 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
            "23/04/11 18:19:06 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1957 ms on ed3eecdcf213 (executor driver) (1/4)\n",
            "23/04/11 18:19:06 INFO HadoopRDD: Input split: file:/content/Salaries.csv:0+350012\n",
            "23/04/11 18:19:06 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 55037\n",
            "23/04/11 18:19:06 INFO PythonRunner: Times: total = 83, boot = -108, init = 124, finish = 67\n",
            "23/04/11 18:19:06 INFO PythonRunner: Times: total = 1205, boot = 673, init = 68, finish = 464\n",
            "23/04/11 18:19:07 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1571 bytes result sent to driver\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (ed3eecdcf213, executor driver, partition 3, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:07 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
            "23/04/11 18:19:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1571 bytes result sent to driver\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 883 ms on ed3eecdcf213 (executor driver) (2/4)\n",
            "23/04/11 18:19:07 INFO HadoopRDD: Input split: file:/content/Salaries.csv:350012+350012\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2871 ms on ed3eecdcf213 (executor driver) (3/4)\n",
            "23/04/11 18:19:07 INFO PythonRunner: Times: total = 139, boot = -712, init = 734, finish = 117\n",
            "23/04/11 18:19:07 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1614 bytes result sent to driver\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 381 ms on ed3eecdcf213 (executor driver) (4/4)\n",
            "23/04/11 18:19:07 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/4_join.py:78) finished in 3.803 s\n",
            "23/04/11 18:19:07 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/11 18:19:07 INFO DAGScheduler: running: Set()\n",
            "23/04/11 18:19:07 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)\n",
            "23/04/11 18:19:07 INFO DAGScheduler: failed: Set()\n",
            "23/04/11 18:19:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/11 18:19:07 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[10] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/11 18:19:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.1 KiB, free 365.5 MiB)\n",
            "23/04/11 18:19:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.5 MiB)\n",
            "23/04/11 18:19:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ed3eecdcf213:32799 (size: 6.5 KiB, free: 366.2 MiB)\n",
            "23/04/11 18:19:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 18:19:07 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[10] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "23/04/11 18:19:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4) (ed3eecdcf213, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5) (ed3eecdcf213, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 4)\n",
            "23/04/11 18:19:07 INFO Executor: Running task 1.0 in stage 1.0 (TID 5)\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Getting 3 (1152.0 B) non-empty blocks including 3 (1152.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 33 ms\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Getting 3 (717.0 B) non-empty blocks including 3 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 28 ms\n",
            "23/04/11 18:19:07 INFO PythonRunner: Times: total = 37, boot = -1614, init = 1650, finish = 1\n",
            "23/04/11 18:19:07 INFO Executor: Finished task 1.0 in stage 1.0 (TID 5). 1783 bytes result sent to driver\n",
            "23/04/11 18:19:07 INFO PythonRunner: Times: total = 57, boot = -541, init = 598, finish = 0\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6) (ed3eecdcf213, executor driver, partition 2, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 294 ms on ed3eecdcf213 (executor driver) (1/4)\n",
            "23/04/11 18:19:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 4). 1783 bytes result sent to driver\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7) (ed3eecdcf213, executor driver, partition 3, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 330 ms on ed3eecdcf213 (executor driver) (2/4)\n",
            "23/04/11 18:19:07 INFO Executor: Running task 3.0 in stage 1.0 (TID 7)\n",
            "23/04/11 18:19:07 INFO Executor: Running task 2.0 in stage 1.0 (TID 6)\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Getting 3 (723.0 B) non-empty blocks including 3 (723.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Getting 3 (817.0 B) non-empty blocks including 3 (817.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n",
            "23/04/11 18:19:08 INFO PythonRunner: Times: total = 49, boot = -31, init = 80, finish = 0\n",
            "23/04/11 18:19:08 INFO Executor: Finished task 3.0 in stage 1.0 (TID 7). 1783 bytes result sent to driver\n",
            "23/04/11 18:19:08 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 127 ms on ed3eecdcf213 (executor driver) (3/4)\n",
            "23/04/11 18:19:08 INFO PythonRunner: Times: total = 49, boot = -33, init = 82, finish = 0\n",
            "23/04/11 18:19:08 INFO Executor: Finished task 2.0 in stage 1.0 (TID 6). 1783 bytes result sent to driver\n",
            "23/04/11 18:19:08 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 166 ms on ed3eecdcf213 (executor driver) (4/4)\n",
            "23/04/11 18:19:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/11 18:19:08 INFO DAGScheduler: ShuffleMapStage 1 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
            "23/04/11 18:19:08 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/11 18:19:08 INFO DAGScheduler: running: Set()\n",
            "23/04/11 18:19:08 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
            "23/04/11 18:19:08 INFO DAGScheduler: failed: Set()\n",
            "23/04/11 18:19:08 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[16] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/11 18:19:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 105.9 KiB, free 365.4 MiB)\n",
            "23/04/11 18:19:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.4 MiB)\n",
            "23/04/11 18:19:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ed3eecdcf213:32799 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/11 18:19:08 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 18:19:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[16] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/11 18:19:08 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "23/04/11 18:19:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8) (ed3eecdcf213, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:08 INFO Executor: Running task 0.0 in stage 2.0 (TID 8)\n",
            "23/04/11 18:19:08 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/11 18:19:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/11 18:19:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/11 18:19:08 INFO ShuffleBlockFetcherIterator: Getting 4 (2.1 KiB) non-empty blocks including 4 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 18:19:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n",
            "23/04/11 18:19:08 INFO PythonRunner: Times: total = 58, boot = -360, init = 418, finish = 0\n",
            "23/04/11 18:19:08 INFO FileOutputCommitter: Saved output of task 'attempt_202304111819036714100715957713928_0016_m_000000_0' to file:/content/4_join.out/_temporary/0/task_202304111819036714100715957713928_0016_m_000000\n",
            "23/04/11 18:19:08 INFO SparkHadoopMapRedUtil: attempt_202304111819036714100715957713928_0016_m_000000_0: Committed\n",
            "23/04/11 18:19:08 INFO Executor: Finished task 0.0 in stage 2.0 (TID 8). 1952 bytes result sent to driver\n",
            "23/04/11 18:19:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 435 ms on ed3eecdcf213 (executor driver) (1/1)\n",
            "23/04/11 18:19:08 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/11 18:19:08 INFO DAGScheduler: ResultStage 2 (runJob at SparkHadoopWriter.scala:83) finished in 0.548 s\n",
            "23/04/11 18:19:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/11 18:19:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "23/04/11 18:19:08 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 5.110313 s\n",
            "23/04/11 18:19:08 INFO SparkHadoopWriter: Start to commit write Job job_202304111819036714100715957713928_0016.\n",
            "23/04/11 18:19:08 INFO SparkHadoopWriter: Write Job job_202304111819036714100715957713928_0016 committed. Elapsed time: 32 ms.\n",
            "23/04/11 18:19:08 INFO SparkUI: Stopped Spark web UI at http://ed3eecdcf213:4040\n",
            "23/04/11 18:19:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/11 18:19:08 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/11 18:19:08 INFO BlockManager: BlockManager stopped\n",
            "23/04/11 18:19:08 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/11 18:19:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/11 18:19:08 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/11 18:19:09 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/11 18:19:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969\n",
            "23/04/11 18:19:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/pyspark-e2e7f5af-c9de-4c72-a442-c3a092711e96\n",
            "23/04/11 18:19:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-64066caf-9c9b-418d-97b6-708eb32de1e7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpkDNQCb__D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a66a4f5-9d64-4f3a-da7f-7b7357b58940"
      },
      "source": [
        "# test locally\n",
        "results = [('join', 'bondsba01,1991,1,PIT,NL,153,153,510,95,149,28,5,25,116,43,13,107,73,25,4,0,13,8,1531991,PIT,NL,bondsba01,2300000'),\n",
        "('join', 'bondsba01,1993,1,SFN,NL,159,159,539,129,181,38,4,46,123,29,12,126,79,43,2,0,7,11,1591993,SFN,NL,bondsba01,4516666'),\n",
        "('join', 'bondsba01,2002,1,SFN,NL,143,143,403,117,149,31,2,46,110,9,2,198,47,68,9,0,2,4,1432002,SFN,NL,bondsba01,15000000'),\n",
        "('join', 'bondsba01,2004,1,SFN,NL,147,147,373,129,135,27,3,45,101,6,1,232,41,120,9,0,3,5,1472004,SFN,NL,bondsba01,18000000'),\n",
        "('join', 'bondsba01,1986,1,PIT,NL,113,113,413,72,92,26,3,16,48,36,7,65,102,2,2,2,2,4,1131986,PIT,NL,bondsba01,60000'),\n",
        "('join', 'bondsba01,1996,1,SFN,NL,158,158,517,122,159,27,3,42,129,40,7,151,76,30,1,0,6,11,1581996,SFN,NL,bondsba01,8416667'),\n",
        "('join', 'bondsba01,1997,1,SFN,NL,159,159,532,123,155,26,5,40,101,37,8,145,87,34,8,0,5,13,1591997,SFN,NL,bondsba01,8666667'),\n",
        "('join', 'bondsba01,1999,1,SFN,NL,102,102,355,91,93,20,2,34,83,15,2,73,62,9,3,0,3,6,1021999,SFN,NL,bondsba01,9381057'),\n",
        "('join', 'bondsba01,1990,1,PIT,NL,151,151,519,104,156,32,3,33,114,52,13,93,83,15,3,0,6,8,1511990,PIT,NL,bondsba01,850000'),\n",
        "('join', 'bondsba01,1994,1,SFN,NL,112,112,391,89,122,18,1,37,81,29,9,74,43,18,6,0,3,3,1121994,SFN,NL,bondsba01,5166666'),\n",
        "('join', 'bondsba01,1995,1,SFN,NL,144,144,506,109,149,30,7,33,104,31,10,120,83,22,5,0,4,12,1441995,SFN,NL,bondsba01,8166666'),\n",
        "('join', 'bondsba01,2003,1,SFN,NL,130,130,390,111,133,22,1,45,90,7,0,148,58,61,10,0,2,7,1302003,SFN,NL,bondsba01,15500000'),\n",
        "('join', 'bondsba01,2007,1,SFN,NL,126,126,340,75,94,14,0,28,66,5,0,132,54,43,3,0,2,13,1262007,SFN,NL,bondsba01,15533970'),\n",
        "('join', 'bondsba01,1987,1,PIT,NL,150,150,551,99,144,34,9,25,59,32,10,54,88,3,3,0,3,4,1501987,PIT,NL,bondsba01,100000'),\n",
        "('join', 'bondsba01,1988,1,PIT,NL,144,144,538,97,152,30,5,24,58,17,11,72,82,14,2,0,2,3,1441988,PIT,NL,bondsba01,220000'),\n",
        "('join', 'bondsba01,1989,1,PIT,NL,159,159,580,96,144,34,6,19,58,32,10,93,93,22,1,1,4,9,1591989,PIT,NL,bondsba01,360000'),\n",
        "('join', 'bondsba01,1992,1,PIT,NL,140,140,473,109,147,36,5,34,103,39,8,127,69,32,5,0,7,9,1401992,PIT,NL,bondsba01,4800000'),\n",
        "('join', 'bondsba01,1998,1,SFN,NL,156,156,552,120,167,44,7,37,122,28,12,130,92,29,8,1,6,15,1561998,SFN,NL,bondsba01,8916667'),\n",
        "('join', 'bondsba01,2000,1,SFN,NL,143,143,480,129,147,28,4,49,106,11,3,117,77,22,3,0,7,6,1432000,SFN,NL,bondsba01,10658826'),\n",
        "('join', 'bondsba01,2001,1,SFN,NL,153,153,476,129,156,32,2,73,137,13,3,177,93,35,9,0,2,5,1532001,SFN,NL,bondsba01,10300000'),\n",
        "('join', 'bondsba01,2005,1,SFN,NL,14,14,42,8,12,1,0,5,10,0,0,9,6,3,0,0,1,0,142005,SFN,NL,bondsba01,22000000'),\n",
        "('join', 'bondsba01,2006,1,SFN,NL,130,130,367,74,99,23,0,26,77,3,0,115,51,38,10,0,1,9,1302006,SFN,NL,bondsba01,19331470')]\n",
        "def test4(lines):\n",
        "  global results\n",
        "  results = [str(x) for x in results]\n",
        "  find_lines = 0\n",
        "  for  line in lines:\n",
        "    if line.strip() in results:\n",
        "        find_lines += 1\n",
        "  if find_lines != 22:\n",
        "      assert False\n",
        "  print('test passed')\n",
        "with open('4_join.out/part-00000') as f:\n",
        "    lines = f.readlines()\n",
        "    test4(lines)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 Execute the same scripts on EMR.\n",
        "\n",
        "* Make sure that you have created an EMR cluster using the instructions in the main readme.\n",
        "* upload the main data to s3"
      ],
      "metadata": {
        "id": "Ed2gw0o387bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup AWS Credentials and fill them here. Make sure you do not save this information back to github"
      ],
      "metadata": {
        "id": "dyeTsQLV-B-u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMEsJXTU__Dx"
      },
      "source": [
        "# Please fill your aws credential information here\n",
        "credentials = {\n",
        "    'region_name': 'us-east-1',\n",
        "    'aws_access_key_id': 'ASIA6CQF7SPNOL6UAOFJ',\n",
        "    'aws_secret_access_key': 'cMMvDZh/1hPipMSJlgB5hX0Nf3Iy2dXa+hE6Ek7C',\n",
        "    'aws_session_token': 'FwoGZXIvYXdzENP//////////wEaDIXDcykaqQX8xRTKOSLOAZnctufpKy0ahlGtM/9i4U2eDWRFOvaG8Xj8gjTtcI+zdH3E8FkD4lfKFKJxKAbqtTGqTvkMF02coCAiaO6eoAX9jhiPwPEzZKgVHjJFBYw4vaHHtBzCvYhi2sOHrFc2BTpHAEhlOEwAmk1MA0OoTZ9JPJo4bVfQfiiHCmpqVxZmSf5FB/lip5rN/bh+QdmH3TlOU1+bQnRV+a5Ixdd3TsEgm0Np7FDE7SvjcQ2G9rA2YRP+iIuIo9YS5OdiJQvmKUALYt8BmRmrUVdLR3l7KOCu1qEGMi2hylSI02FS7J7cHkRPxtFifq0aJohaczxBFBRvytWQOgviikIpvwBd33/slFs='\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRJjVwtO__Dx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10d2b50-bf94-4dee-e3e8-46ea4a5bf807"
      },
      "source": [
        "!pip install boto3\n",
        "import boto3, json\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.110-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.110\n",
            "  Downloading botocore-1.29.110-py3-none-any.whl (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.110->boto3) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.110->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.110->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.110 botocore-1.29.110 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Data to S3"
      ],
      "metadata": {
        "id": "Mss_SpU89_eo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-9aO5-hmAv4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MzsIwij__Dy"
      },
      "source": [
        "# upload tweets dataset to S3, please replace the bucket name and object keys with yours\n",
        "s3.upload_file(Filename='nashville-tweets-2019-01-28', Bucket='vandy-bd', Key='hw6/nashville-tweets-2019-01-28')\n",
        "s3.upload_file(Filename='Batting.csv', Bucket='vandy-bd', Key='hw6/Batting.csv')\n",
        "s3.upload_file(Filename='Salaries.csv', Bucket='vandy-bd', Key='hw6/Salaries.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lljwHyOy__D2"
      },
      "source": [
        "# replae with your EMR cluster ID\n",
        "CLUSTER_ID = 'j-145LUR2LF753J'\n",
        "\n",
        "def submit_job(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCzOGzTj__D1"
      },
      "source": [
        "# upload script to S3\n",
        "s3.upload_file(Filename='2_group.py', Bucket='vandy-bd', Key='hw6/2_group.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoqkfcEn__D1"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='2_group', pyfile_uri='s3://vandy-bd/hw6/2_group.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm7iAdlL__D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e35f2d19-d43c-4ae8-efc8-433bb26ed76b"
      },
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/2_group.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bd', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test2(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VUV5ZIp__D2"
      },
      "source": [
        "# upload script to S3 - Make sure that the S3 bucket name is changed to your own bucket\n",
        "s3.upload_file(Filename='3_days.py', Bucket='vandy-bd', Key='hw6/3_days.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFWrjexF__D2"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='3_days', pyfile_uri='s3://vandy-bd/hw6/3_days.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1eFnxXZ__D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f192a253-314b-4678-c9e7-db3fdac0781c"
      },
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/3_days.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bd', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test3(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload script to S3 - Make sure that the S3 bucket name is changed to your own bucket\n",
        "s3.upload_file(Filename='4_join.py', Bucket='vandy-bd', Key='hw6/4_join.py')"
      ],
      "metadata": {
        "id": "Xxf7WSbSDcsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkQiNiXW__D4"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='4_join', pyfile_uri='s3://vandy-bd/hw6/4_join.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqyNRexw__D4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5aa5302-ae2d-4ab0-bc77-a6a4cc5e430a"
      },
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/4_join.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bd', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test4(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test passed\n"
          ]
        }
      ]
    }
  ]
}